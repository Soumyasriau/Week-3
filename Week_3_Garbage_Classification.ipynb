{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOckp+5YbonjY+aQnjgZ57J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumyasriau/Week-3/blob/main/Week_3_Garbage_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2Coe6Z2qCKz"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "asdasdasasdas_garbage_classification_path = kagglehub.dataset_download('asdasdasasdas/garbage-classification')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "z9-suj9RbOKn"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oB92GumlbPz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-09-29T18:08:32.295501Z",
          "iopub.execute_input": "2024-09-29T18:08:32.295969Z",
          "iopub.status.idle": "2024-09-29T18:08:35.101916Z",
          "shell.execute_reply.started": "2024-09-29T18:08:32.295923Z",
          "shell.execute_reply": "2024-09-29T18:08:35.100569Z"
        },
        "trusted": true,
        "id": "3M4pErtXbOKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries\n"
      ],
      "metadata": {
        "id": "PV1NEAqqbOKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:25:47.925722Z",
          "iopub.execute_input": "2024-09-29T18:25:47.926922Z",
          "iopub.status.idle": "2024-09-29T18:25:47.933111Z",
          "shell.execute_reply.started": "2024-09-29T18:25:47.926869Z",
          "shell.execute_reply": "2024-09-29T18:25:47.931903Z"
        },
        "trusted": true,
        "id": "XyO0JaQubOKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set directories and parameters\n"
      ],
      "metadata": {
        "id": "5e-C-clHbOKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/kaggle/input/garbage-classification/Garbage classification/Garbage classification\"\n",
        "img_size = (150, 150)\n",
        "batch_size = 32\n",
        "epochs = 10\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:25:48.858888Z",
          "iopub.execute_input": "2024-09-29T18:25:48.859407Z",
          "iopub.status.idle": "2024-09-29T18:25:48.865449Z",
          "shell.execute_reply.started": "2024-09-29T18:25:48.859359Z",
          "shell.execute_reply": "2024-09-29T18:25:48.86401Z"
        },
        "trusted": true,
        "id": "f5oPjURIbOKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation and Preprocessing\n"
      ],
      "metadata": {
        "id": "w_jUXkjEbOKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2  # 80% training, 20% validation\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:26:22.71575Z",
          "iopub.execute_input": "2024-09-29T18:26:22.716286Z",
          "iopub.status.idle": "2024-09-29T18:26:22.722351Z",
          "shell.execute_reply.started": "2024-09-29T18:26:22.71624Z",
          "shell.execute_reply": "2024-09-29T18:26:22.721143Z"
        },
        "trusted": true,
        "id": "R-yKZRpTbOKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create training and validation generators\n"
      ],
      "metadata": {
        "id": "XNv2_O_nbOKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:26:25.138752Z",
          "iopub.execute_input": "2024-09-29T18:26:25.139219Z",
          "iopub.status.idle": "2024-09-29T18:26:25.3184Z",
          "shell.execute_reply.started": "2024-09-29T18:26:25.139173Z",
          "shell.execute_reply": "2024-09-29T18:26:25.317145Z"
        },
        "trusted": true,
        "id": "81JI9YZzbOKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the CNN model\n"
      ],
      "metadata": {
        "id": "QlBoOpx2bOKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:27:33.166492Z",
          "iopub.execute_input": "2024-09-29T18:27:33.166972Z",
          "iopub.status.idle": "2024-09-29T18:27:33.260978Z",
          "shell.execute_reply.started": "2024-09-29T18:27:33.166929Z",
          "shell.execute_reply": "2024-09-29T18:27:33.259835Z"
        },
        "trusted": true,
        "id": "Ay_HkpN4bOKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile the model\n"
      ],
      "metadata": {
        "id": "q5YYMESJbOKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:27:53.769092Z",
          "iopub.execute_input": "2024-09-29T18:27:53.769543Z",
          "iopub.status.idle": "2024-09-29T18:27:53.787638Z",
          "shell.execute_reply.started": "2024-09-29T18:27:53.769502Z",
          "shell.execute_reply": "2024-09-29T18:27:53.786224Z"
        },
        "trusted": true,
        "id": "n-YeeJVBbOKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n"
      ],
      "metadata": {
        "id": "O3EvFnTEbOKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:28:57.173438Z",
          "iopub.execute_input": "2024-09-29T18:28:57.173867Z",
          "iopub.status.idle": "2024-09-29T18:40:33.281906Z",
          "shell.execute_reply.started": "2024-09-29T18:28:57.173824Z",
          "shell.execute_reply": "2024-09-29T18:40:33.280411Z"
        },
        "trusted": true,
        "id": "EElZkOW6bOKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "MbbW-Wn0bOKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy and loss over epochs\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:40:33.28443Z",
          "iopub.execute_input": "2024-09-29T18:40:33.284879Z",
          "iopub.status.idle": "2024-09-29T18:40:33.858232Z",
          "shell.execute_reply.started": "2024-09-29T18:40:33.284834Z",
          "shell.execute_reply": "2024-09-29T18:40:33.857014Z"
        },
        "trusted": true,
        "id": "1BeA1_n1bOKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model\n"
      ],
      "metadata": {
        "id": "LOgyM0_8bOKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {val_acc}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T18:40:41.96525Z",
          "iopub.execute_input": "2024-09-29T18:40:41.965715Z",
          "iopub.status.idle": "2024-09-29T18:40:47.280089Z",
          "shell.execute_reply.started": "2024-09-29T18:40:41.965655Z",
          "shell.execute_reply": "2024-09-29T18:40:47.278753Z"
        },
        "trusted": true,
        "id": "GsUWSsLqbOKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ds9mBcgfIWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Importing NumPy for numerical operations and array manipulations\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib for plotting graphs and visualizations\n",
        "import seaborn as sns  # Importing Seaborn for statistical data visualization, built on top of Matplotlib\n",
        "import tensorflow as tf  # Importing TensorFlow for building and training machine learning models\n",
        "from tensorflow import keras  # Importing Keras, a high-level API for TensorFlow, to simplify model building\n",
        "from tensorflow.keras import Layer  # Importing Layer class for creating custom layers in Keras\n",
        "from tensorflow.keras.models import Sequential  # Importing Sequential model for building neural networks layer-by-layer\n",
        "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks  # Importing various modules for layers, optimizers, and callbacks in Keras\n",
        "from sklearn.utils.class_weight import compute_class_weight  # Importing function to compute class weights for imbalanced datasets\n",
        "from tensorflow.keras.applications import EfficientNetV2B2  # Importing EfficientNetV2S model for transfer learning\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Importing functions to evaluate model performance\n",
        "import gradio as gr  # Importing Gradio for creating interactive web interfaces for machine learning models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "z-yQbwHfgcyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "with zipfile.ZipFile(\"archive (4).zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")"
      ],
      "metadata": {
        "id": "eyzFCPprgc7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"dataset/TrashType_Image_Dataset\"\n",
        "print(os.path.exists(dataset_dir))  # Should return True\n"
      ],
      "metadata": {
        "id": "OOgUIwqMkeMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = r\"C:\\Users\\argha\\Downloads\\archive\\TrashType_Image_Dataset.zip\"\n",
        "image_size = (124, 124)\n",
        "batch_size = 32\n",
        "seed = 42"
      ],
      "metadata": {
        "id": "hRRN4wawkiZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"dataset/TrashType_Image_Dataset\"\n",
        "print(os.path.exists(dataset_dir))  # Should return True"
      ],
      "metadata": {
        "id": "DfNRjUwMkvc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    shuffle = True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "BRumx939lJXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    shuffle = True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "val_class= val_ds.class_names"
      ],
      "metadata": {
        "id": "SudPzmiYlN5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of batches in the validation dataset\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "\n",
        "# Split the validation dataset into two equal parts:\n",
        "# First half becomes the test dataset\n",
        "test_ds = val_ds.take(val_batches // 2)\n",
        "\n",
        "# Second half remains as the validation dataset\n",
        "val_dat = val_ds.skip(val_batches // 2)\n",
        "\n",
        "# Optimize test dataset by caching and prefetching to improve performance\n",
        "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "print(train_ds.class_names)\n",
        "print(val_class)\n",
        "print(len(train_ds.class_names))"
      ],
      "metadata": {
        "id": "DwUsT6_XlOEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(12):\n",
        "    ax = plt.subplot(4, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(train_ds.class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "uvhb6mGBlONz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_distribution(dataset, class_names):\n",
        "    total = 0\n",
        "    counts = {name: 0 for name in class_names}\n",
        "\n",
        "    for _, labels in dataset:\n",
        "        for label in labels.numpy():\n",
        "            class_name = class_names[label]\n",
        "            counts[class_name] += 1\n",
        "            total += 1\n",
        "\n",
        "    for k in counts:\n",
        "        counts[k] = round((counts[k] / total) * 100, 2)  # Convert to percentage\n",
        "    return counts\n",
        "# Function to plot class distribution\n",
        "def simple_bar_plot(dist, title):\n",
        "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "class_names = train_ds.class_names\n",
        "\n",
        "# Get class distributions\n",
        "train_dist = count_distribution(train_ds, class_names)\n",
        "val_dist = count_distribution(val_ds, class_names)\n",
        "test_dist = count_distribution(test_ds, class_names)\n",
        "overall_dist = {}\n",
        "for k in class_names:\n",
        "    overall_dist[k] = round((train_dist[k] + val_dist[k]) / 2, 2)\n",
        "\n",
        "print(train_dist)\n",
        "print(val_dist)\n",
        "print(test_dist)\n",
        "print(overall_dist)"
      ],
      "metadata": {
        "id": "H4JmoYMtl3F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show visualizations\n",
        "simple_bar_plot(train_dist, \"Training Set Class Distribution (%)\")\n",
        "simple_bar_plot(val_dist, \"Validation Set Class Distribution (%)\")\n",
        "simple_bar_plot(test_dist, \"Test Set Class Distribution (%)\")\n",
        "simple_bar_plot(overall_dist, \"Overall Class Distribution (%)\")"
      ],
      "metadata": {
        "id": "AOgGoO95l3JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count class occurrences and prepare label list\n",
        "class_counts = {i: 0 for i in range(len(class_names))}\n",
        "all_labels = []\n",
        "\n",
        "for images, labels in train_ds:\n",
        "    for label in labels.numpy():\n",
        "        class_counts[label] += 1\n",
        "        all_labels.append(label)\n",
        "\n",
        "# Compute class weights (index aligned)\n",
        "class_weights_array = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(class_names)),\n",
        "    y=all_labels\n",
        ")\n",
        "\n",
        "# Create dictionary mapping class index to weight\n",
        "class_weights = {i: w for i, w in enumerate(class_weights_array)}\n",
        "print(\"Class Counts:\", class_counts)\n",
        "print(\"Class Weights:\", class_weights)"
      ],
      "metadata": {
        "id": "svIYnH4kl3YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),\n",
        "])\n",
        "base_model = EfficientNetV2B2(include_top=False, input_shape=(124, 124, 3),include_preprocessing=True, weights='imagenet')\n",
        "\n",
        "\n",
        "#  Freeze early layers (to retain general pretrained features)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:  # You can adjust this number\n",
        "    layer.trainable = False\n",
        "    model = Sequential([\n",
        "    layers.Input(shape=(124, 124, 3)),\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "R382vrM0l3hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',            # Metric to monitor (validation loss here)\n",
        "    patience=3,                   # Number of epochs to wait after last improvement before stopping\n",
        "    restore_best_weights=True     # After stopping, restore the model weights from the epoch with the best val_loss\n",
        ")\n",
        "epochs = 15  # Number of times the model will go through the entire dataset\n",
        "history = model.fit(\n",
        "    train_ds,                # Training dataset used to adjust model weights\n",
        "    validation_data=val_ds,   # Validation dataset to monitor performance on unseen data\n",
        "    epochs=epochs,           # Number of training cycles, referencing the variable set earlier\n",
        "    class_weight=class_weights,  # Handles class imbalances by assigning appropriate weights\n",
        "    batch_size=32,           # Number of samples processed in each training step\n",
        "    callbacks=[early]        # Implements early stopping to prevent unnecessary training\n",
        ")"
      ],
      "metadata": {
        "id": "DHSLHp82lOZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bnbc19SkxO3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "TCimMx4JxO6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']          # Extract training accuracy from history\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']             # Extract training loss from history\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(len(acc))             # Define range for epochs based on accuracy length\n",
        "\n",
        "plt.figure(figsize=(10,8))                 # Set overall figure size for visualization\n",
        "\n",
        "plt.subplot(1,2,1)                         # Create first subplot (1 row, 2 columns, position 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')       # Plot training accuracy\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy') # Plot validation accuracy\n",
        "plt.legend(loc='lower right')              # Place legend in lower-right corner\n",
        "plt.title('Training vs Validation Accuracy') # Add title for accuracy plot\n",
        "\n",
        "plt.subplot(1,2,2)                         # Create second subplot (1 row, 2 columns, position 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')         # Plot training loss\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')   # Plot validation loss\n",
        "plt.legend(loc='upper right')              # Place legend in upper-right corner\n",
        "plt.title('Training vs Validation Loss')   # Add title for loss plot\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2VczurJ7xO98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds_eval)\n",
        "print(f'Test accuracy is{accuracy:.4f}, Test loss is {loss:.4f}')"
      ],
      "metadata": {
        "id": "mbevXxyexhbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract true labels from all batches in the test dataset\n",
        "y_true = np.concatenate([y.numpy() for x, y in test_ds_eval], axis=0)  # Convert Tensor labels to NumPy array and concatenate them\n",
        "\n",
        "# Get predictions as probabilities from the model\n",
        "y_pred_probs = model.predict(test_ds_eval)  # Predict class probabilities for each sample in the test dataset\n",
        "\n",
        "# Convert probabilities to predicted class indices\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)  # Select the class with the highest probability for each sample\n",
        "\n",
        "# Compute the confusion matrix to evaluate classification performance\n",
        "cm = confusion_matrix(y_true, y_pred)  # Generate confusion matrix comparing true labels to predicted labels\n",
        "\n",
        "# Print metrics to assess model performance\n",
        "print(cm)  # Display confusion matrix\n",
        "print(classification_report(y_true, y_pred))\n",
        "plt.figure(figsize=(10,8))  # Set figure size for better visualization\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d',  # Create heatmap using confusion matrix\n",
        "            xticklabels=class_names,  # Set class names for x-axis (predicted labels)\n",
        "            yticklabels=class_names,  # Set class names for y-axis (true labels)\n",
        "            cmap='Blues')  # Use a blue colormap for better contrast\n",
        "\n",
        "plt.xlabel('Predicted')  # Label x-axis as Predicted classes\n",
        "plt.ylabel('True')  # Label y-axis as True classes\n",
        "plt.title('Confusion Matrix')  # Add title to the heatmap\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8P6Z-zs8xh8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Size used during training\n",
        "IMG_SIZE = (124, 124)\n",
        "\n",
        "# Use trained model (if not already loaded)\n",
        "# model = load_model(\"your_model_path.h5\")  # Uncomment this if you saved and want to reload later\n",
        "\n",
        "# Use existing class names from training\n",
        "# class_names = train_ds.class_names  # Already defined during dataset creation\n",
        "\n",
        "def predict_image(image_path):\n",
        "    # Read and convert to RGB (OpenCV loads in BGR by default)\n",
        "    img_bgr = cv2.imread(image_path)\n",
        "    if img_bgr is None:\n",
        "        print(f\"Error: Could not read image from {image_path}\")\n",
        "        return\n",
        "\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize to match model input\n",
        "    img_resized = cv2.resize(img_rgb, IMG_SIZE)\n",
        "\n",
        "    # Normalize and expand dims to simulate batch\n",
        "    img_array = np.expand_dims(img_resized / 255.0, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(img_array)\n",
        "    class_index = np.argmax(prediction[0])\n",
        "    confidence = np.max(prediction[0]) * 100\n",
        "    predicted_label = class_names[class_index]\n",
        "\n",
        "    # Output text\n",
        "    result_text = f\"Class: {predicted_label}, Confidence: {confidence:.2f}%\"\n",
        "    print(result_text)\n",
        "\n",
        "    # Display with matplotlib\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(result_text, fontsize=12)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Upload image from local system\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "extracted_dir = \"uploaded_images\"\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "for file_name in uploaded.keys():\n",
        "    if file_name.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_dir)\n",
        "        os.remove(file_name) # Remove the uploaded zip file after extraction\n",
        "\n",
        "# Predict for each extracted image\n",
        "for root, _, files in os.walk(extracted_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(root, file)\n",
        "            predict_image(image_path)"
      ],
      "metadata": {
        "id": "KAxyejjstS8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31ae2389"
      },
      "source": [
        "# Upload image from local system\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "extracted_dir = \"uploaded_images\"\n",
        "os.makedirs(extracted_dir, exist_ok=True)\n",
        "for file_name in uploaded.keys():\n",
        "    if file_name.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_dir)\n",
        "        os.remove(file_name) # Remove the uploaded zip file after extraction\n",
        "\n",
        "# Predict for each extracted image\n",
        "for root, _, files in os.walk(extracted_dir):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(root, file)\n",
        "            predict_image(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}